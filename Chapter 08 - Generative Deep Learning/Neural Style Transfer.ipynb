{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "Neural style transfer consist of applying the style of a reference image to a targe image while conserving the conent of the target.\n",
    "\n",
    "![](neural-style-transfer.jpeg)\n",
    "\n",
    "In this context, *style* means texture, colors, and visual patterns in the image, at caripus spatial scales; and the *content* is the higher-level macrostructure of the image.\n",
    "\n",
    "The key notion behind implementing style transfer is the sam idea that's central to all deep learning algorithms: you define a loss function to specify what you want to achieve, in this case *applying the style of a reference image to a targe image while conserving the conent of the target*. Then an appropiate loss function to minimize would be the following:\n",
    "\n",
    "```\n",
    "loss = distance(style(reference_image) - style(generated_image)) + \n",
    "        distance(content(original_image) - content(generated_image))\n",
    "```\n",
    "\n",
    "- `distance`: L2 norm\n",
    "- `content`: function that takes an image and computes a representation of its content (CNNs can encode content) \n",
    "- `style`: function that takes an image and computes a representations of its style (CNNs can encode style)\n",
    "\n",
    "So ->\n",
    "\n",
    "- Preserve content by maintaining similar high-level layer activations between the target content and the generated one.\n",
    "- Preserve style by maintaining similar *correlations* within activation for both lower-level layers and high-level-layers. Feature correlations capture *textures*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Algorithm of Artistic Style (2015) implementation\n",
    "\n",
    "1. Set up anetwork that computes VGG19 layers activations for style reference image, target image and generated image\n",
    "2. Use the layers activations computed over this three images to define the loss function\n",
    "3. Set up a gradient-descent process to minimize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dd.jpg\t\t\t\t 'Neural Style Transfer.ipynb'\r\n",
      " DeepDream.ipynb\t\t  neural-style-transfer.jpeg\r\n",
      "'dream_at_scale_(169, 255).png'   original_dd.jpeg\r\n",
      "'dream_at_scale_(237, 357).png'   original-st.jpg\r\n",
      "'dream_at_scale_(332, 500).png'   style.jpg\r\n",
      " final_dream.png\t\t 'Text generation with LSTM.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_path = 'original-st.jpg'\n",
    "style_reference_image_path = 'style.jpg'\n",
    "\n",
    "width, height = load_img(target_image_path).size\n",
    "\n",
    "# Calculate dimentions of the generated image\n",
    "# All images shares 400px of height\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height) # keep proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = load_img(img_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939 # Reverse transformation of data standarization\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1] # Convert image from BGR to RGB\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guillem/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "input_tensor = K.concatenate([\n",
    "    target_image,\n",
    "    style_reference_image,\n",
    "    combination_image\n",
    "], axis=0)\n",
    "\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "def gram_matrix(x):\n",
    "    # Inner product of the feature maps of given layer\n",
    "    # Inner product can be understood as representing a map of the correlation between the layer's features\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    return K.dot(features, K.transpose(features))\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "# Regularization loss\n",
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, 1:, :img_width - 1, :]\n",
    "    )\n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, :img_height -1, 1:, :]\n",
    "    )\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {layer.name: layer.output for layer in model.layers}\n",
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1\n",
    "content_weight = .025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "loss = K.variable(0.)\n",
    "layer_features = output_dict[content_layer]\n",
    "target_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight* content_loss(target_image_features, combination_features)\n",
    "\n",
    "for l in style_layers:\n",
    "    layer_features = output_dict[l]\n",
    "    combination_image_features = layer_features[2, :, :, :]\n",
    "    style_features = layer_features[1, :, :, :]\n",
    "    loss += (style_weight / len(style_layers)) * style_loss(style_features, combination_image_features)\n",
    "    \n",
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll set up the gradient descent process. In the original paper, optimization is performed using the L-BFGS algorithm, so that's what we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)[0]\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter for using L-BFGS algorithm in scipy\n",
    "class Evaluator(object):\n",
    "    # This class allows fetching loss and gradients separately\n",
    "    def __init__(self):\n",
    "        self.loss_val = None\n",
    "        self.grads_val = None\n",
    "        \n",
    "    def loss(self, x):\n",
    "        assert self.loss_val is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        self.loss_val, self.grads_val = fetch_loss_and_grads([x])\n",
    "        self.grads_val = self.grads_val.flatten().astype('float64')\n",
    "        return self.loss_val\n",
    "    \n",
    "    def grads(self, x):\n",
    "        assert self.loss_val is not None\n",
    "        grads_val = np.copy(self.grads_val)\n",
    "        self.loss_val = None\n",
    "        self.grads_val = None\n",
    "        return grads_val\n",
    "    \n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(639600,)\n",
      "Start of iteration 0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "import time\n",
    "\n",
    "result_prefix = 'my_result'\n",
    "iterations = 20\n",
    "\n",
    "x = preprocess_image(target_image_path)\n",
    "x = x.flatten()\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n",
    "                                     x,\n",
    "                                     fprime=evaluator.grads,\n",
    "                                     maxfun=20)\n",
    "    print('current_loss value:', min_val)\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = deprocess_image(img)\n",
    "    fname = result_prefix + 'at_iteration_{}.png'.format(i)\n",
    "    imsave(fname, img)\n",
    "    print('Image saved as', fname)\n",
    "    end_time = time.time()\n",
    "    print('Iteration {} completed in {}s'.format(i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
